= Remote Provider
:navtitle: Remote Provider

== Overview

The remote provider runs Ragas evaluation in a separate Kubeflow Pipelines environment. This provides better isolation, scalability, and is ideal for production deployments.

== Architecture

The remote provider architecture includes:

[mermaid]
----
graph TB
    subgraph LSS ["ðŸŸ¦ Llama Stack Server"]
        style LSS stroke-dasharray: 5 5
        A[Llama Stack Server] --> B[Remote Provider]
    end

    subgraph KFP ["â˜ï¸ Cloud"]
        style KFP fill:#e1f5fe,stroke:#01579b,stroke-width:2px
        B --> C[Kubeflow Pipelines API]
        C --> D[Kubernetes Pod]
        D --> E[Ragas Evaluation]
        E --> F[S3 Results Storage]
        style F fill:#ff9800,stroke:#e65100,color:#fff
    end

    F -.->|"Fetch Results"| A
----

=== Components

==== Llama Stack Server
The main server that receives requests and coordinates with the remote provider.

==== Remote Provider
The provider implementation that creates and manages Kubeflow Pipelines for evaluation.

==== Kubeflow Pipelines API
The interface for creating, submitting, and monitoring evaluation pipelines.

==== Kubernetes Pod
Isolated containers where the actual RAGAS evaluation runs.

==== S3 Results Storage
S3-compatible storage system for evaluation results and artifacts. The Llama Stack server fetches results from S3 after pipeline completion.

== Installation

=== Prerequisites

* Python 3.12 or later
* https://docs.astral.sh/uv/[uv] package manager
* Kubernetes cluster with Kubeflow Pipelines installed
* Access to the Kubeflow Pipelines API
* Container registry access for custom images

==== Kubeflow Pipelines Server

Ensure you have a running Kubeflow Pipelines installation. You can verify access with:

[,python]
----
import kfp
client = kfp.Client(host='<your-kfp-endpoint>')
print("Connection successful")
----

=== One-liner setup for the impatient

- Running this command will start a Llama Stack server with the Ragas provider installed, and use the minimal distribution that is the `distribution` directory.
- Note that we are asking for the `[remote,distro]` dependency groups (more info below).
- Also note that, for this one-liner to work, you will need to have your environment variables set up (see <<_environment_variables>>).

[,bash]
----
uv run --with llama-stack-provider-ragas[remote,distro] llama stack run distribution/run-remote.yaml
----

=== Installing with uv

To get started with uv, create a virtual environment and install from PyPI:

[,bash]
----
uv venv --python=3.12
source .venv/bin/activate
uv pip install llama-stack-provider-ragas
----

=== Development setup

If you're planning to contribute and make modifications to the code, ensure that you clone the repository and set it up as an editable install:

[,bash]
----
git clone https://github.com/trustyai-explainability/llama-stack-provider-ragas
cd llama-stack-provider-ragas
uv pip install -e .
----

=== Optional dependencies for remote provider

The package includes several optional dependency groups:

[cols="1,3"]
|===
|Group |Description

|`dev`
|Development dependencies including testing tools, linting, and type checking

|`remote`
|Dependencies for the Kubeflow Pipelines-enabled remote provider

|`distro`
|Dependencies to use the provided minimal Llama Stack distribution under `distribution/`
|===

==== Installing with optional dependencies

[,bash]
----
# For development (includes all dependencies)
uv pip install -e ".[dev]"

# For remote provider (includes the KFP dependencies)
uv pip install -e ".[remote]"

# For using the sample distribution
uv pip install -e ".[distro]"
----

== Configuration

=== Environment Variables

Create a `.env` file in the project root with the following variables:

[,properties]
----
# Required for both inline and remote
EMBEDDING_MODEL=all-MiniLM-L6-v2

# Llama Stack server URL for remote provider
KUBEFLOW_LLAMA_STACK_URL=<your-llama-stack-url>

# Kubeflow Pipelines endpoint
KUBEFLOW_PIPELINES_ENDPOINT=<your-kfp-endpoint>

# Kubernetes namespace for Kubeflow
KUBEFLOW_NAMESPACE=<your-namespace>

# Container image for remote execution
KUBEFLOW_BASE_IMAGE=quay.io/diegosquayorg/my-ragas-provider-image:latest
----

=== Environment Variable Details

`EMBEDDING_MODEL`::
The embedding model to use for RAGAS evaluation. This should match a model available in your Llama Stack configuration.

`KUBEFLOW_LLAMA_STACK_URL`::
The URL of the Llama Stack server that the remote provider will use for LLM generations and embeddings. If running Llama Stack locally, you can use https://ngrok.com/[ngrok] to expose it to the remote provider.

`KUBEFLOW_PIPELINES_ENDPOINT`::
The endpoint URL for your Kubeflow Pipelines server. You can get this by running:
+
[,bash]
----
kubectl get routes -A | grep -i pipeline
----

`KUBEFLOW_NAMESPACE`::
The name of the data science project where the Kubeflow Pipelines server is running.

`KUBEFLOW_BASE_IMAGE`::
The container image used to run the Ragas evaluation in the remote provider. See the `Containerfile` in the repository root for details on building a custom image.

=== Distribution Configuration

The repository includes a sample Llama Stack distribution configuration that uses Ollama as a provider for inference and embeddings.

The remote provider is setup in the following lines of the `run-remote.yaml`:

[,yaml]
----
eval:
  - provider_id: trustyai_ragas
    provider_type: remote::trustyai_ragas
    module: llama_stack_provider_ragas.remote
    config:
      embedding_model: ${env.EMBEDDING_MODEL}
      kubeflow_config:
        results_s3_prefix: ${env.KUBEFLOW_RESULTS_S3_PREFIX}
        s3_credentials_secret_name: ${env.KUBEFLOW_S3_CREDENTIALS_SECRET_NAME}
        pipelines_endpoint: ${env.KUBEFLOW_PIPELINES_ENDPOINT}
        namespace: ${env.KUBEFLOW_NAMESPACE}
        llama_stack_url: ${env.KUBEFLOW_LLAMA_STACK_URL}
        base_image: ${env.KUBEFLOW_BASE_IMAGE}
----

To run with the sample distribution:

[,bash]
----
dotenv run uv run llama stack run distribution/run-remote.yaml
----

== Usage

=== Starting the Server

Start the Llama Stack server with the included distribution configuration:

[,bash]
----
dotenv run uv run llama stack run distribution/run-remote.yaml
----

This will start a server with the remote Ragas evaluation provider available.

=== Basic Evaluation Workflow

1. **Prepare Your Data**: Ensure your evaluation data is in the format expected by Ragas and Llama Stack
2. **Submit Evaluation**: Use the Llama Stack eval API to submit your evaluation request
3. **Pipeline Execution**: Remote provider creates and executes Kubeflow Pipeline
4. **Monitor Progress**: Track evaluation progress through Kubeflow APIs
5. **Collect Results**: Results are automatically collected and returned

=== Example Workflow

The repository includes demonstration examples in the `demos/` directory showing how to use the remote provider.
